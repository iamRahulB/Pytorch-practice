{"metadata":{"interpreter":{"hash":"5eed43bc279c158ceae0b8f32e5e090b01ddf39e56bb88f0a575c0b2a9f28e1a"},"kernelspec":{"display_name":"Python 3.10.0 64-bit","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.0"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport torch\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom torch import nn","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# in order to make a model we need to make a dataset in which there should be valid relationship between the x nd y so that model can be made otherwise it will be model\n#  with just a random prediction \n\n#  linear regresion data - Y=mx+c \n\nweight=0.7\nbias=0.3\nstart =0\nend=1\nstep=0.02\n\nx=torch.arange(start,end,step).unsqueeze(dim=1)\n\ny= weight*x+bias","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"(x.size(),\"\\n\",  y.size())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_split=int(0.8* len(x))\n\nX_train=x[0:train_split]\ny_train=y[0:train_split]\nX_test=x[train_split:]\ny_test=y[train_split:]\n\nlen(X_train),len(y_train), len(X_test),len(y_test)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.scatter(X_train,y_train )\n\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class LinearRegressionModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.weight=nn.Parameter(torch.randn(1,requires_grad=True,dtype=torch.float))\n        self.bias=nn.Parameter(torch.randn(1,requires_grad=True,dtype=torch.float))\n\n    def forward(self,x):\n        return self.weight*x+self.bias","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.manual_seed(42)\nmodel=LinearRegressionModel()\n\nwith torch.inference_mode():    # this removes any track of gradient while predicting\n\n    y_pred=model(X_test)\n\n# with torch.no_grad():              can be done but inference model is preferred more. \n#     y_pred=model(X_test)\n\n\nprint(model.state_dict())\n\ny_pred","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss_fn=nn.MSELoss()\n\noptim=torch.optim.SGD(params=model.parameters(),lr=0.01)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epochs=200\n\nfor epoch in range(epochs):\n    model.train()\n\n    y_pred=model(X_train)\n\n    loss=loss_fn(y_pred,y_train)\n    optim.zero_grad()\n\n    loss.backward()\n\n    optim.step()\n\n\n    model.eval()   # turn of gradient track\n    \n    with torch.inference_mode():\n        test_pred=model(X_test)\n\n        test_loss=loss_fn(test_pred,y_test)   \n\n    if epoch % 10==0:\n        print(f\"Epoch :{epoch} | Loss :{loss} | Test Loss :{test_loss}\")  \n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.state_dict()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# with torch.inference_mode():       # torch.inference_mode() is used when we are not trining model but testing model\n#     y_pred","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model(torch.from_numpy(np.array([[2]])))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.scatter(X_train,y_pred.detach().numpy())\nplt.scatter(X_train,y_train )\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pickle","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(model,\"model.pkl\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mod=torch.load('model.pkl')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mod.state_dict()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#  in below code we are saving model state dict and after saving we need to create a object of same class LinearRegressionModel so it will be empty first then\n#  we will load state dict from previous model\n# directly we can save also no worries.\n#  but while loading we need class also to work that.\n\nsave_state_dict=torch.save(model.state_dict(),\"model_state_dict.pkl\")\nmodel_1=LinearRegressionModel()\n\n# model_1.load_state_dict(torch.load('model.pkl').state_dict())\n\nmodel_1.load_state_dict(torch.load('model_state_dict.pkl'))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_1.state_dict()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Putting all together and try to code myself\n","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch import nn\nimport numpy as np\nimport matplotlib.pyplot as plt","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#  Creating data\n#  y=mx+c\n\nstart=0\nstep=0.02\nweight=0.7\nend=1\nbias=0.3\n\nX=torch.arange(start,end,step).unsqueeze(dim=1)\nY= weight * X +bias","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#  splitting \n\npercent=0.8 \n\nX_train=X[:int(len(X)*0.8)]\ny_train=Y[:int(len(Y)*0.8)]\nX_test=X[int(len(X)*0.8):]\ny_test=Y[int(len(Y)*0.8):]\n\nlen(X_train),len(y_train),len(X_test),len(y_test)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class LinearModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        # self.weight=nn.Parameter(torch.randn(1,requires_grad=True,dtype=torch.float))\n        # self.bias=nn.Parameter(torch.randn(1,requires_grad=True,dtype=torch.float))\n\n        self.linear=nn.Linear(1,1)\n\n    def forward(self,x):\n        return   self.linear(x)            #self.weight*x +self.bias","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.manual_seed(42)\nmodel=LinearModel()\n\nloss_fn=nn.MSELoss()\n\noptim=torch.optim.SGD(params=model.parameters(),lr=0.01)\n\nepochs=200\n\nfor epoch in range(epochs):\n\n    model.train()\n    y_pred=model(X_train)\n    loss=loss_fn(y_pred,y_train)\n    optim.zero_grad()\n    loss.backward()\n    optim.step()\n\n\n    model.eval()\n    with torch.inference_mode():\n        test_pred=model(X_test)\n        test_loss=loss_fn(test_pred,y_test)   \n\n    if epoch % 10==0:\n        print(f\"Epoch: {epoch} | Loss: {loss:.4f} | Test Loss: {test_loss:.4f}\")\n \n\n    \n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.state_dict()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.Tensor()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}