{"metadata":{"interpreter":{"hash":"5eed43bc279c158ceae0b8f32e5e090b01ddf39e56bb88f0a575c0b2a9f28e1a"},"kernelspec":{"display_name":"Python 3.10.0 64-bit","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.0"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch import nn\nimport numpy as np\nimport sklearn\nfrom sklearn.datasets import make_circles\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\n\nimport torch_directml\nimport torch.optim as optim\n\nfrom sklearn.metrics import accuracy_score\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X,y=make_circles(1000,noise=0.03,random_state=42)\n\nlen(X),len(y)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df=pd.DataFrame({\"x1\":X[:,0], \"x2\":X[:,1],\"y\":y})\ndf","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.scatterplot(x=X[:,0],y=X[:,1],hue=y)\nplt.xlabel(\"x1\")\nplt.ylabel(\"x2\")\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"type(X),X.dtype","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#   Here we are changing type from float 64 of numpy to float 32 of torch default type \nX=torch.from_numpy(X).type(torch.float)\ny=torch.from_numpy(y).type(torch.float)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train,X_test,y_train,y_test=train_test_split(X,y,random_state=42,test_size=0.2)\n\nlen(X_train), len(X_test),len(y_train),len(y_test)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ndml=torch_directml.device()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Notes\n\n1. **Loss Calculation**: The loss should be calculated using the raw outputs from the model, not the rounded predictions.\n2. **Accuracy Calculation**: The accuracy should be calculated using the rounded predictions.\n3. **Model Initialization**: The model should be initialized before defining the optimizer.\n/","metadata":{}},{"cell_type":"code","source":"# Calculate accuracy (a classification metric)\ndef accuracy_fn(y_true, y_pred):\n    correct = torch.eq(y_true, y_pred).sum().item() # torch.eq() calculates where two tensors are equal\n    acc = (correct / len(y_pred)) * 100 \n    return acc","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CircleModelV0(nn.Module):\n    def __init__(self):\n        super().__init__()\n        # 2. Create 2 nn.Linear layers capable of handling X and y input and output shapes\n        self.layer_1 = nn.Linear(in_features=2, out_features=5) # takes in 2 features (X), produces 5 features\n        self.layer_2 = nn.Linear(in_features=5, out_features=1) # takes in 5 features, produces 1 feature (y)\n    \n    # 3. Define a forward method containing the forward pass computation\n    def forward(self, x):\n        # Return the output of layer_2, a single feature, the same shape as y\n        return self.layer_2(self.layer_1(x)) # computation goes through layer_1 first then the output of layer_1 goes through layer_2\n\n# 4. Create an instance of the model and send it to target device\nmodel_0 = CircleModelV0()\nmodel_0\n\n# Create a loss function\n# loss_fn = nn.BCELoss() # BCELoss = no sigmoid built-in\nloss_fn = nn.BCEWithLogitsLoss() # BCEWithLogitsLoss = sigmoid built-in\n\n# Create an optimizer\noptimizer = torch.optim.SGD(params=model_0.parameters(), \n                            lr=0.1)\n\n\ntorch.manual_seed(42)\n\n# Set the number of epochs\nepochs = 1000\n\n# Put data to target device\nX_train, y_train = X_train.to(device), y_train\nX_test, y_test = X_test.to(device), y_test\n\n# Build training and evaluation loop\nfor epoch in range(epochs):\n    ### Training\n    model_0.train()\n\n    # 1. Forward pass (model outputs raw logits)\n    y_logits = model_0(X_train).squeeze() # squeeze to remove extra `1` dimensions, this won't work unless model and data are on same device \n    y_pred = torch.round(torch.sigmoid(y_logits)) # turn logits -> pred probs -> pred labls\n  \n    # 2. Calculate loss/accuracy\n    # loss = loss_fn(torch.sigmoid(y_logits), # Using nn.BCELoss you need torch.sigmoid()\n    #                y_train) \n    loss = loss_fn(y_logits, # Using nn.BCEWithLogitsLoss works with raw logits\n                   y_train) \n    acc = accuracy_fn(y_true=y_train, \n                      y_pred=y_pred) \n\n    # 3. Optimizer zero grad\n    optimizer.zero_grad()\n\n    # 4. Loss backwards\n    loss.backward()\n\n    # 5. Optimizer step\n    optimizer.step()\n\n    ### Testing\n    model_0.eval()\n    with torch.inference_mode():\n        # 1. Forward pass\n        test_logits = model_0(X_test).squeeze() \n        test_pred = torch.round(torch.sigmoid(test_logits))\n        # 2. Caculate loss/accuracy\n        test_loss = loss_fn(test_logits,\n                            y_test)\n        test_acc = accuracy_fn(y_true=y_test,\n                               y_pred=test_pred)\n\n    # Print out what's happening every 10 epochs\n    if epoch % 10 == 0:\n        print(f\"Epoch: {epoch} | Loss: {loss:.5f}, Accuracy: {acc:.2f}% | Test loss: {test_loss:.5f}, Test acc: {test_acc:.2f}%\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from helper_functions import plot_predictions, plot_decision_boundary\n\nplt.figure(figsize=(12, 6))\nplt.subplot(1, 2, 1)\nplt.title(\"Train\")\nplot_decision_boundary(model_0, X_train, y_train)\nplt.subplot(1, 2, 2)\nplt.title(\"Test\")\nplot_decision_boundary(model_0, X_test, y_test)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Coding with non linear model so we can make good classification boundries","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch import nn\n\nimport torch.optim as optim\nimport numpy as np\nfrom sklearn.datasets import make_circles\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X,y=make_circles(n_samples=1000,\nnoise=0.03,\nrandom_state=42\n )\n\nplt.scatter(X[:,0],X[:,1],c=y)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train,X_test,y_train,y_test=train_test_split(X,y,random_state=42, test_size=0.2)\n\nX_train=torch.from_numpy(X_train).type(dtype=torch.float)\nX_test=torch.from_numpy(X_test).type(dtype=torch.float)\ny_train=torch.from_numpy(y_train).type(dtype=torch.float)\ny_test=torch.from_numpy(y_test).type(dtype=torch.float)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(X_train),len(X_test),len(y_train),len(y_test)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train[0:3], y_test[0:10]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nn.ReLU","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#  building model with non linearity\n\n\nclass NonLinearModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        self.sequential = nn.Sequential(\n    nn.Linear(in_features=2, out_features=32),\n    nn.ReLU(),\n    nn.Linear(32, 64),\n    nn.ReLU(),  \n    nn.Linear(64, 1)\n)\n\n    def forward(self,x):\n\n        return self.sequential(x)\n\n\nmodel_3=NonLinearModel()\nmodel_3","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.manual_seed(42)\n\nloss_fn=nn.BCEWithLogitsLoss()\n\noptimizer=optim.Adam(params=model_3.parameters(),lr=0.01)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epochs=20\n\nfor epoch in range(epochs):\n\n    model_3.train()\n\n    y_logits=model_3(X_train).squeeze()\n    y_pred=torch.round(torch.sigmoid(y_logits))\n\n    loss=loss_fn(y_logits,y_train)\n\n    acc=accuracy_fn(y_train,y_pred)\n\n    optimizer.zero_grad()\n\n    loss.backward()\n\n    optimizer.step()\n    model_3.eval()\n\n    with torch.inference_mode():\n        #  test loss and pred and dont need to update gradient so normal code\n\n        y_test_logits=model_3(X_test).squeeze()\n\n        y_test_pred=torch.round(torch.sigmoid(y_test_logits))\n\n        test_loss=loss_fn(y_test_logits,y_test)\n        test_acc=accuracy_fn(y_test,y_test_pred)\n\n    if epoch%10==0:\n\n        print(f\"Epoch: {epoch} | Loss: {loss:.4f} | accu: {acc}% | Test Loss: {test_loss:.4f} | Test_ACC: {test_acc}%\")\n\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from helper_functions import plot_predictions, plot_decision_boundary\n\nplt.figure(figsize=(12, 6))\nplt.subplot(1, 2, 1)\nplt.title(\"Train\")\nplot_decision_boundary(model_3, X_train, y_train)\nplt.subplot(1, 2, 2)\nplt.title(\"Test\")\nplot_decision_boundary(model_3, X_test, y_test)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"acc, test_acc","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_3(X_train).squeeze().shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}
