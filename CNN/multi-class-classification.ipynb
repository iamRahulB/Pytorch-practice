{"metadata":{"interpreter":{"hash":"5eed43bc279c158ceae0b8f32e5e090b01ddf39e56bb88f0a575c0b2a9f28e1a"},"kernelspec":{"display_name":"Python 3.10.0 64-bit","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.0"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.optim as optim\nfrom torch import nn\nfrom sklearn.datasets import make_blobs\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X,y=make_blobs(n_features=2,n_samples=1000,centers=4,cluster_std=1.5,random_state=42)\n\nX=torch.from_numpy(X).type(torch.float)\ny=torch.from_numpy(y).type(torch.long)\n\nX_train,X_test,y_train,y_test=train_test_split(X,y,random_state=42,test_size=0.2)\n\nlen(X_train), len(X_test), len(y_train), len(y_test)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(10,7))\n\nplt.scatter(X[:,0],X[:,1],c=y)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"128/4","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MultiClassClassification(nn.Module):\n    def __init__(self,input_features,output_features):\n        super().__init__()\n\n        self.sequential=nn.Sequential(\n            nn.Linear(input_features,16),\n            nn.ReLU(),\n            nn.Linear(16,32),\n            nn.ReLU(),\n            nn.Linear(32,output_features)\n        )\n\n    def forward(self,x):\n        return self.sequential(x)\n\nmodel=MultiClassClassification(input_features=X_train.shape[1],output_features=len(y_train.unique()))\n\nmodel","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def accuracy_fn(y_true, y_pred):\n    correct = torch.eq(y_true, y_pred).sum().item() # torch.eq() calculates where two tensors are equal\n    acc = (correct / len(y_pred)) * 100 \n    return acc","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss_fn=nn.CrossEntropyLoss()\n\noptimizer=optim.Adam(params=model.parameters(),lr=0.01)\n\nepochs=20\n\ntorch.manual_seed(42)\n\nfor epoch in range(epochs):\n\n    model.train()\n\n    y_pred_logits=model(X_train)\n    y_pred_proba=torch.softmax(y_pred_logits,dim=1)\n\n    y_pred=torch.argmax(y_pred_proba,dim=1,)\n\n    loss=loss_fn(y_pred_logits,y_train)\n\n    train_acc=accuracy_fn(y_train,y_pred)\n\n    optimizer.zero_grad()\n\n    loss.backward()\n\n    optimizer.step()\n\n    model.eval()\n\n    with torch.inference_mode():\n\n        test_logits=model(X_test)\n\n        test_proba=torch.softmax(test_logits,dim=1)\n\n        test_pred=torch.argmax(test_proba,dim=1)\n\n        test_loss=loss_fn(test_logits,y_test)\n\n        test_acc=accuracy_fn(y_test,test_pred)\n\n    \n    if epoch%5==0:\n\n        print(f\"Epoch: {epoch} | Loss: {loss:.4f} | acc: {train_acc}% | Test Loss: {test_loss:.4f} | Test Acc: {test_acc}%\")\n    \n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from helper_functions import plot_decision_boundary\n\n\nplt.figure(figsize=(15,7))\nplt.subplot(1,2,1)\nplot_decision_boundary(model,X_train,y_train)\nplt.subplot(1,2,2)\nplot_decision_boundary(model,X_test,y_test)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train.dtype","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##  More Classification metrics ","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import precision_score, recall_score,accuracy_score,classification_report,confusion_matrix","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#  precision\n#  recall \n#  accuracy score\n#  f1 score\n#  confusion metrics\n#  classification report \n\n\nwith torch.inference_mode():\n    logits=model(X_train)\n\n    proba=torch.softmax(logits,dim=1)\n    preds=torch.argmax(proba,dim=1)\n\n    print(classification_report(y_pred=preds,y_true=y_train))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accuracy_score","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#   what to remember\n\n#  errors will be in datatype, first check what function except in datatype,\n#  when predicted it gives logits, probabilities we use softmax, but we need to place dim on which softmax is applied.\n#  if multiclass then we need to take index of that class which has high proba so used argmax\n#  loss expects datatype of float 32 and int64 or we can say long.\n#  before going ahead always check needed data type of imported function\n# \n# also important thing to remember, that check loss function expect logits or class of precticted output\n# \n","metadata":{},"execution_count":null,"outputs":[]}]}
